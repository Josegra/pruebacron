name: Ejecutar Scraper de FBref y Guardar CSV en Repositorio

on:
  schedule:
    # Ejecuta todos los días a las 10:30 UTC (que son las 12:30 CEST)
    - cron: '30 10 * * *'
  workflow_dispatch: # Permite la ejecución manual

jobs:
  scrape_and_commit:
    runs-on: ubuntu-latest
    permissions:
      contents: write # Permiso para escribir en el contenido del repositorio

    steps:
      - name: Checkout del repositorio
        uses: actions/checkout@v4 # Eliminada la sección 'with:' de aquí

      - name: Configurar Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install pandas lxml html5lib

      - name: Ejecutar script standard_scrape.py
        run: python standard_scrape.py

      - name: Commit y Push del archivo CSV
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          # Mensaje del commit. "[skip ci]" evita bucles de CI si tienes otros workflows.
          # Puedes cambiar el mensaje a algo como "chore: Actualizar datos de jugadores (CSV) [skip ci]"
          commit_message: "регулярно обновляется [skip ci] datos de jugadores (CSV)"
          branch: main # O tu rama principal
          file_pattern: ./data/big5_player_stats.csv
          commit_user_name: GitHub Actions Bot
          commit_user_email: actions@github.com
          commit_author: GitHub Actions Bot <actions@github.com>

      # Opcional: Descomenta si también quieres un artefacto descargable
      # - name: Subir artefacto CSV (Opcional)
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: estadisticas-jugadores-big5
      #     path: ./data/big5_player_stats.csv
      #     if-no-files-found: error
